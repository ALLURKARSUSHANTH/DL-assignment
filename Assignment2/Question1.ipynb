{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHQOjtj4ox5R",
        "outputId": "b0e57c4f-9dce-41d7-d09f-aa8e66759b42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-04-20 05:20:38--  https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.101.207, 142.251.2.207, 142.250.141.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.101.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2008340480 (1.9G) [application/x-tar]\n",
            "Saving to: ‘dakshina_dataset_v1.0.tar.1’\n",
            "\n",
            "dakshina_dataset_v1 100%[===================>]   1.87G   261MB/s    in 9.4s    \n",
            "\n",
            "2025-04-20 05:20:48 (204 MB/s) - ‘dakshina_dataset_v1.0.tar.1’ saved [2008340480/2008340480]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\n",
        "!tar -xf dakshina_dataset_v1.0.tar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwTCDWbvpApa",
        "outputId": "23a4b317-b53c-4fb8-b151-10725431ea9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hi.translit.sampled.dev.tsv   hi.translit.sampled.train.tsv\n",
            "hi.translit.sampled.test.tsv\n"
          ]
        }
      ],
      "source": [
        "ls dakshina_dataset_v1.0/hi/lexicons/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "piZTA6n7z8eH",
        "outputId": "c3c31be3-c108-4bf6-f79a-68aad6022bf5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample training data:\n",
            "  devanagari     latin  count\n",
            "0         अं        an      3\n",
            "1    अंकगणित  ankganit      3\n",
            "2       अंकल     uncle      4\n",
            "3      अंकुर     ankur      4\n",
            "4     अंकुरण   ankuran      3\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_21\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"functional_21\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_45      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_layer_46      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_26        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,728</span> │ input_layer_45[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_27        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,224</span> │ input_layer_46[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)      │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>),     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">98,816</span> │ embedding_26[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>),      │            │                   │\n",
              "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)]      │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)      │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">98,816</span> │ embedding_27[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ lstm_30[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>],    │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ lstm_30[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>]     │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)]             │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">66</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,514</span> │ lstm_31[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_45      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_layer_46      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_26        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │      \u001b[38;5;34m1,728\u001b[0m │ input_layer_45[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_27        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │      \u001b[38;5;34m4,224\u001b[0m │ input_layer_46[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_30 (\u001b[38;5;33mLSTM\u001b[0m)      │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m),     │     \u001b[38;5;34m98,816\u001b[0m │ embedding_26[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m),      │            │                   │\n",
              "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)]      │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_31 (\u001b[38;5;33mLSTM\u001b[0m)      │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,     │     \u001b[38;5;34m98,816\u001b[0m │ embedding_27[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│                     │ \u001b[38;5;34m128\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │ lstm_30[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m],    │\n",
              "│                     │ \u001b[38;5;34m128\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │ lstm_30[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m2\u001b[0m]     │\n",
              "│                     │ \u001b[38;5;34m128\u001b[0m)]             │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_13 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m66\u001b[0m)  │      \u001b[38;5;34m8,514\u001b[0m │ lstm_31[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">212,098</span> (828.51 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m212,098\u001b[0m (828.51 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">212,098</span> (828.51 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m212,098\u001b[0m (828.51 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - accuracy: 0.7087 - loss: 1.2900 - val_accuracy: 0.7667 - val_loss: 0.8528\n",
            "Epoch 2/30\n",
            "\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.7704 - loss: 0.8399 - val_accuracy: 0.8092 - val_loss: 0.6861\n",
            "Epoch 3/30\n",
            "\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.8143 - loss: 0.6574 - val_accuracy: 0.8586 - val_loss: 0.4870\n",
            "Epoch 4/30\n",
            "\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - accuracy: 0.8647 - loss: 0.4581 - val_accuracy: 0.8939 - val_loss: 0.3507\n",
            "Epoch 5/30\n",
            "\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.8983 - loss: 0.3328 - val_accuracy: 0.9109 - val_loss: 0.2860\n",
            "Epoch 6/30\n",
            "\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.9155 - loss: 0.2728 - val_accuracy: 0.9206 - val_loss: 0.2523\n",
            "Epoch 7/30\n",
            "\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.9263 - loss: 0.2379 - val_accuracy: 0.9276 - val_loss: 0.2315\n",
            "Epoch 8/30\n",
            "\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.9341 - loss: 0.2115 - val_accuracy: 0.9327 - val_loss: 0.2157\n",
            "Epoch 9/30\n",
            "\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.9399 - loss: 0.1927 - val_accuracy: 0.9360 - val_loss: 0.2044\n",
            "Epoch 10/30\n",
            "\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.9449 - loss: 0.1761 - val_accuracy: 0.9380 - val_loss: 0.1975\n",
            "Epoch 11/30\n",
            "\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.9483 - loss: 0.1647 - val_accuracy: 0.9398 - val_loss: 0.1932\n",
            "Epoch 12/30\n",
            "\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.9520 - loss: 0.1528 - val_accuracy: 0.9414 - val_loss: 0.1874\n",
            "Epoch 13/30\n",
            "\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.9541 - loss: 0.1454 - val_accuracy: 0.9422 - val_loss: 0.1840\n",
            "Epoch 14/30\n",
            "\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - accuracy: 0.9573 - loss: 0.1360 - val_accuracy: 0.9432 - val_loss: 0.1802\n",
            "Epoch 15/30\n",
            "\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - accuracy: 0.9594 - loss: 0.1304 - val_accuracy: 0.9434 - val_loss: 0.1795\n",
            "Epoch 16/30\n",
            "\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - accuracy: 0.9612 - loss: 0.1234 - val_accuracy: 0.9433 - val_loss: 0.1786\n",
            "Epoch 17/30\n",
            "\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - accuracy: 0.9627 - loss: 0.1186 - val_accuracy: 0.9447 - val_loss: 0.1782\n",
            "Epoch 18/30\n",
            "\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.9655 - loss: 0.1106 - val_accuracy: 0.9440 - val_loss: 0.1810\n",
            "Epoch 19/30\n",
            "\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.9664 - loss: 0.1076 - val_accuracy: 0.9446 - val_loss: 0.1794\n",
            "Epoch 20/30\n",
            "\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - accuracy: 0.9682 - loss: 0.1017 - val_accuracy: 0.9447 - val_loss: 0.1818\n",
            "Epoch 21/30\n",
            "\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.9695 - loss: 0.0972 - val_accuracy: 0.9450 - val_loss: 0.1802\n",
            "Epoch 22/30\n",
            "\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.9709 - loss: 0.0935 - val_accuracy: 0.9450 - val_loss: 0.1817\n",
            "Epoch 23/30\n",
            "\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.9718 - loss: 0.0897 - val_accuracy: 0.9450 - val_loss: 0.1835\n",
            "Epoch 24/30\n",
            "\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.9726 - loss: 0.0867 - val_accuracy: 0.9438 - val_loss: 0.1878\n",
            "Epoch 25/30\n",
            "\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.9741 - loss: 0.0823 - val_accuracy: 0.9448 - val_loss: 0.1858\n",
            "Epoch 26/30\n",
            "\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.9749 - loss: 0.0796 - val_accuracy: 0.9455 - val_loss: 0.1882\n",
            "Epoch 27/30\n",
            "\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.9765 - loss: 0.0752 - val_accuracy: 0.9433 - val_loss: 0.1950\n",
            "Epoch 28/30\n",
            "\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.9766 - loss: 0.0738 - val_accuracy: 0.9438 - val_loss: 0.1946\n",
            "Epoch 29/30\n",
            "\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.9779 - loss: 0.0704 - val_accuracy: 0.9440 - val_loss: 0.1940\n",
            "Epoch 30/30\n",
            "\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - accuracy: 0.9786 - loss: 0.0685 - val_accuracy: 0.9447 - val_loss: 0.1965\n",
            "Test Accuracy: 0.9439\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\n",
            "Sample 1:\n",
            "Input (Latin): a n k\n",
            "Target (Devanagari): अ ं क\n",
            "Predicted (Devanagari): अंक\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\n",
            "Sample 2:\n",
            "Input (Latin): a n k a\n",
            "Target (Devanagari): अ ं क\n",
            "Predicted (Devanagari): अंका\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\n",
            "Sample 3:\n",
            "Input (Latin): a n k i t\n",
            "Target (Devanagari): अ ं क ि त\n",
            "Predicted (Devanagari): अंकित\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\n",
            "Sample 4:\n",
            "Input (Latin): a n a k o n\n",
            "Target (Devanagari): अ ं क ो ं\n",
            "Predicted (Devanagari): अनकों\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\n",
            "Sample 5:\n",
            "Input (Latin): a n k h o n\n",
            "Target (Devanagari): अ ं क ो ं\n",
            "Predicted (Devanagari): अनखों\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, LSTM, GRU, SimpleRNN, Embedding, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Load Hindi data\n",
        "def load_hindi_data():\n",
        "    train_path = \"dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\"\n",
        "    dev_path = \"dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv\"\n",
        "    test_path = \"dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv\"\n",
        "\n",
        "    train_data = pd.read_csv(train_path, sep='\\t', header=None,\n",
        "                           names=['devanagari', 'latin', 'count'])\n",
        "    dev_data = pd.read_csv(dev_path, sep='\\t', header=None,\n",
        "                         names=['devanagari', 'latin', 'count'])\n",
        "    test_data = pd.read_csv(test_path, sep='\\t', header=None,\n",
        "                          names=['devanagari', 'latin', 'count'])\n",
        "\n",
        "    return train_data, dev_data, test_data\n",
        "\n",
        "train_data, dev_data, test_data = load_hindi_data()\n",
        "\n",
        "# Show some samples\n",
        "print(\"Sample training data:\")\n",
        "print(train_data.head())\n",
        "\n",
        "def preprocess_data(train_data, dev_data, test_data, max_sequence_length=20):\n",
        "    # First, clean the data by removing any rows with NaN values\n",
        "    train_data = train_data.dropna()\n",
        "    dev_data = dev_data.dropna()\n",
        "    test_data = test_data.dropna()\n",
        "\n",
        "    # Combine all data for vocabulary creation\n",
        "    all_latin = pd.concat([train_data['latin'], dev_data['latin'], test_data['latin']])\n",
        "    all_devanagari = pd.concat([train_data['devanagari'], dev_data['devanagari'], test_data['devanagari']])\n",
        "\n",
        "    # Convert to string type to ensure we don't have any numeric values\n",
        "    all_latin = all_latin.astype(str)\n",
        "    all_devanagari = all_devanagari.astype(str)\n",
        "\n",
        "    # Create character-level tokenizers\n",
        "    latin_tokenizer = Tokenizer(char_level=True, lower=False)\n",
        "    latin_tokenizer.fit_on_texts(all_latin)\n",
        "\n",
        "    devanagari_tokenizer = Tokenizer(char_level=True, lower=False)\n",
        "    devanagari_tokenizer.fit_on_texts(all_devanagari)\n",
        "\n",
        "    # Add start and end tokens for decoder sequences\n",
        "    devanagari_tokenizer.word_index['<start>'] = len(devanagari_tokenizer.word_index) + 1\n",
        "    devanagari_tokenizer.word_index['<end>'] = len(devanagari_tokenizer.word_index) + 1\n",
        "\n",
        "    # Convert texts to sequences\n",
        "    def process_sequences(texts, tokenizer, max_len):\n",
        "        # Ensure all texts are strings\n",
        "        texts = [str(text) for text in texts]\n",
        "        seq = tokenizer.texts_to_sequences(texts)\n",
        "        seq = pad_sequences(seq, maxlen=max_len, padding='post')\n",
        "        return seq\n",
        "\n",
        "    # Process input (Latin) sequences\n",
        "    X_train = process_sequences(train_data['latin'], latin_tokenizer, max_sequence_length)\n",
        "    X_dev = process_sequences(dev_data['latin'], latin_tokenizer, max_sequence_length)\n",
        "    X_test = process_sequences(test_data['latin'], latin_tokenizer, max_sequence_length)\n",
        "\n",
        "    # Process target (Devanagari) sequences with start/end tokens\n",
        "    def process_target_sequences(texts, tokenizer, max_len):\n",
        "        # Ensure all texts are strings\n",
        "        texts = [str(text) for text in texts]\n",
        "        seq = tokenizer.texts_to_sequences(texts)\n",
        "        # Add start and end tokens\n",
        "        seq = [[tokenizer.word_index['<start>']] + s + [tokenizer.word_index['<end>']] for s in seq]\n",
        "        seq = pad_sequences(seq, maxlen=max_len+2, padding='post')  # +2 for start/end tokens\n",
        "        return seq\n",
        "\n",
        "    y_train = process_target_sequences(train_data['devanagari'], devanagari_tokenizer, max_sequence_length)\n",
        "    y_dev = process_target_sequences(dev_data['devanagari'], devanagari_tokenizer, max_sequence_length)\n",
        "    y_test = process_target_sequences(test_data['devanagari'], devanagari_tokenizer, max_sequence_length)\n",
        "\n",
        "    # Create decoder input (shifted by one) and output data\n",
        "    decoder_input_train = y_train[:, :-1]\n",
        "    decoder_output_train = y_train[:, 1:]\n",
        "\n",
        "    decoder_input_dev = y_dev[:, :-1]\n",
        "    decoder_output_dev = y_dev[:, 1:]\n",
        "\n",
        "    decoder_input_test = y_test[:, :-1]\n",
        "    decoder_output_test = y_test[:, 1:]\n",
        "\n",
        "    # One-hot encode the output\n",
        "    def one_hot_encode(sequences, vocab_size):\n",
        "        return np.array([tf.keras.utils.to_categorical(s, num_classes=vocab_size) for s in sequences])\n",
        "\n",
        "    vocab_size = len(devanagari_tokenizer.word_index) + 1  \n",
        "    \n",
        "    decoder_output_train = one_hot_encode(decoder_output_train, vocab_size)\n",
        "    decoder_output_dev = one_hot_encode(decoder_output_dev, vocab_size)\n",
        "    decoder_output_test = one_hot_encode(decoder_output_test, vocab_size)\n",
        "\n",
        "    return (X_train, decoder_input_train, decoder_output_train,\n",
        "            X_dev, decoder_input_dev, decoder_output_dev,\n",
        "            X_test, decoder_input_test, decoder_output_test,\n",
        "            latin_tokenizer, devanagari_tokenizer)\n",
        "\n",
        "(X_train, decoder_input_train, decoder_output_train,\n",
        " X_dev, decoder_input_dev, decoder_output_dev,\n",
        " X_test, decoder_input_test, decoder_output_test,\n",
        " latin_tokenizer, devanagari_tokenizer) = preprocess_data(train_data, dev_data, test_data)\n",
        "\n",
        "def build_seq2seq_model(input_vocab_size, target_vocab_size, embedding_dim=64,\n",
        "                       hidden_units=128, cell_type='lstm'):\n",
        "    # Encoder\n",
        "    encoder_inputs = Input(shape=(None,))\n",
        "    encoder_embedding = Embedding(input_vocab_size, embedding_dim)(encoder_inputs)\n",
        "\n",
        "    # Choose RNN cell\n",
        "    if cell_type == 'lstm':\n",
        "        encoder_rnn = LSTM(hidden_units, return_state=True)\n",
        "        encoder_outputs, state_h, state_c = encoder_rnn(encoder_embedding)\n",
        "        encoder_states = [state_h, state_c]\n",
        "    elif cell_type == 'gru':\n",
        "        encoder_rnn = GRU(hidden_units, return_state=True)\n",
        "        encoder_outputs, state_h = encoder_rnn(encoder_embedding)\n",
        "        encoder_states = [state_h]\n",
        "    else:  # SimpleRNN\n",
        "        encoder_rnn = SimpleRNN(hidden_units, return_state=True)\n",
        "        encoder_outputs, state_h = encoder_rnn(encoder_embedding)\n",
        "        encoder_states = [state_h]\n",
        "\n",
        "    # Decoder\n",
        "    decoder_inputs = Input(shape=(None,))\n",
        "    decoder_embedding = Embedding(target_vocab_size, embedding_dim)(decoder_inputs)\n",
        "\n",
        "    if cell_type == 'lstm':\n",
        "        decoder_rnn = LSTM(hidden_units, return_sequences=True, return_state=True)\n",
        "        decoder_outputs, _, _ = decoder_rnn(decoder_embedding, initial_state=encoder_states)\n",
        "    elif cell_type == 'gru':\n",
        "        decoder_rnn = GRU(hidden_units, return_sequences=True, return_state=True)\n",
        "        decoder_outputs, _ = decoder_rnn(decoder_embedding, initial_state=encoder_states)\n",
        "    else:\n",
        "        decoder_rnn = SimpleRNN(hidden_units, return_sequences=True, return_state=True)\n",
        "        decoder_outputs, _ = decoder_rnn(decoder_embedding, initial_state=encoder_states)\n",
        "\n",
        "    decoder_dense = Dense(target_vocab_size, activation='softmax')\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "    # Training model\n",
        "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "    # Inference models\n",
        "    encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "    decoder_state_inputs = [Input(shape=(hidden_units,)) for _ in encoder_states]\n",
        "    decoder_outputs, *decoder_states = decoder_rnn(\n",
        "        decoder_embedding, initial_state=decoder_state_inputs)\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "    decoder_model = Model(\n",
        "        [decoder_inputs] + decoder_state_inputs,\n",
        "        [decoder_outputs] + decoder_states)\n",
        "\n",
        "    return model, encoder_model, decoder_model\n",
        "\n",
        "# Build the model\n",
        "input_vocab_size = len(latin_tokenizer.word_index) + 1\n",
        "target_vocab_size = len(devanagari_tokenizer.word_index) + 1\n",
        "\n",
        "model, encoder_model, decoder_model = build_seq2seq_model(\n",
        "    input_vocab_size, target_vocab_size,\n",
        "    embedding_dim=64, hidden_units=128, cell_type='lstm')\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    [X_train, decoder_input_train],\n",
        "    decoder_output_train,\n",
        "    batch_size=64,\n",
        "    epochs=30,\n",
        "    validation_data=([X_dev, decoder_input_dev], decoder_output_dev),\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "def decode_sequence(input_seq, encoder_model, decoder_model,\n",
        "                   latin_tokenizer, devanagari_tokenizer, max_length=20):\n",
        "    # Encode input\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    target_seq[0, 0] = devanagari_tokenizer.word_index['<start>']\n",
        "\n",
        "    reverse_target_char_index = {i: char for char, i in devanagari_tokenizer.word_index.items()}\n",
        "\n",
        "    decoded_sentence = []\n",
        "    for _ in range(max_length):\n",
        "        output_tokens, *states_value = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        decoded_sentence.append(sampled_char)\n",
        "\n",
        "        if sampled_char == '<end>':\n",
        "            break\n",
        "\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "    return ''.join([c for c in decoded_sentence if c not in ['<start>', '<end>']])\n",
        "\n",
        "# Evaluate on test set\n",
        "test_loss, test_acc = model.evaluate([X_test, decoder_input_test], decoder_output_test, verbose=0)\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "for i in range(5):\n",
        "    input_seq = X_test[i:i+1]\n",
        "    decoded = decode_sequence(\n",
        "        input_seq, encoder_model, decoder_model,\n",
        "        latin_tokenizer, devanagari_tokenizer)\n",
        "\n",
        "    original_input = latin_tokenizer.sequences_to_texts([X_test[i]])[0]\n",
        "    original_target = devanagari_tokenizer.sequences_to_texts([decoder_input_test[i]])[0]\n",
        "\n",
        "    print(f\"\\nSample {i+1}:\")\n",
        "    print(f\"Input (Latin): {original_input}\")\n",
        "    print(f\"Target (Devanagari): {original_target}\")\n",
        "    print(f\"Predicted (Devanagari): {decoded}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
